{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from MinecraftSequencePredict import MinecraftSequencePredict\n",
    "import Dataset\n",
    "from Dataset import MinecraftBlockData, custom_collate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "#      Hyperparameters\n",
    "# -----------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = 251\n",
    "d_model = 768\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dim_feed_forward = 2048 * 2\n",
    "\n",
    "src_shape  = (5,5,5)\n",
    "tgt_shape  = (1,5,5)\n",
    "tgt_offset = (5,0,0)\n",
    "# Model Definition\n",
    "model = MinecraftSequencePredict(vocab_size, \n",
    "                                 d_model, \n",
    "                                 nhead, \n",
    "                                 num_encoder_layers, \n",
    "                                 num_decoder_layers, \n",
    "                                 dim_feed_forward,\n",
    "                                 src_shape, \n",
    "                                 tgt_shape, \n",
    "                                 tgt_offset, \n",
    "                                 device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Training Parameters\n",
    "# ------------------------\n",
    "num_epochs = 40\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "\n",
    "\n",
    "learning_rate = 1e-5\n",
    "gamma = 0.1\n",
    "step_size = 1\n",
    "\n",
    "eps = 1e-9\n",
    "dropout = 0.1\n",
    "\n",
    "num_workers = 4\n",
    "\n",
    "n_data = 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Dataset and DataLoader\n",
    "# ----------------------\n",
    "\n",
    "path = '../Datasets/Complete_Datasets/Minecraft6_5_5/data/'\n",
    "\n",
    "files = Dataset.get_filenames(path, int(n_data))\n",
    "\n",
    "train_filenames, test_filenames = train_test_split(files, test_size = 0.1, random_state=42)\n",
    "\n",
    "train_data = MinecraftBlockData(path, train_filenames)\n",
    "\n",
    "test_data = MinecraftBlockData(path, test_filenames)\n",
    "\n",
    "training_dataloader = DataLoader(train_data, batch_size, shuffle, num_workers=num_workers, collate_fn=custom_collate)\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size, shuffle, num_workers=num_workers, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Learning Rate: 1e-05\n",
      "Epoch 1 Training loss: 76.27\n",
      "\n",
      "Epoch 2, Learning Rate: 5e-06\n",
      "Epoch 2 Training loss: 74.09\n",
      "Improvements: Train: 2.18|\n",
      "\n",
      "Epoch 3, Learning Rate: 2.5e-06\n",
      "Epoch 3 Training loss: 74.27\n",
      "Improvements: Train: -0.18|\n",
      "\n",
      "Epoch 4, Learning Rate: 1.25e-06\n",
      "Epoch 4 Training loss: 74.27\n",
      "Improvements: Train: -0.01|\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), tgt\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     23\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\hmhor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hmhor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=eps)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "epoch_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    epoch_loss = 0\n",
    "    validation_loss = 0\n",
    "    k=0\n",
    "    for i, data in enumerate(training_dataloader):\n",
    "        k+=1\n",
    "        src = data['src'].to(device)\n",
    "        tgt = data['tgt'].to(device)\n",
    "        \n",
    "        output = model(src, tgt) # Batch Size x Tgt Sequence Length x Vocab Size\n",
    "        \n",
    "        # Batch Size x Tgt Sequence Length x Vocab Size --> (Batch Size * Tgt Sequence Length x Vocab Size)\n",
    "        loss = loss_fn(output.view(-1, vocab_size), tgt.view(-1))\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    # for i, data in enumerate(test_dataloader):\n",
    "    #     src = data['src'].to(device)\n",
    "    #     tgt = data['tgt'].to(device)\n",
    "        \n",
    "    #     output = model(src, tgt) # Batch Size x Tgt Sequence Length x Vocab Size\n",
    "        \n",
    "    #     loss = loss_fn(output.view(-1, vocab_size), tgt.view(-1))\n",
    "    #     validation_loss += loss\n",
    "        \n",
    "    epoch_losses.append(epoch_loss)\n",
    "    validation_losses.append(validation_loss)\n",
    "    \n",
    "    print(f'\\nEpoch {epoch + 1}, Learning Rate: {scheduler.get_last_lr()[0]}')\n",
    "    print(f'Epoch {epoch + 1} Training loss: {epoch_loss:.2f}')\n",
    "    # print(f'Epoch {epoch + 1} Validation loss: {validation_loss:.2f}\\n')\n",
    "    if(epoch > 0):\n",
    "        print(f\"Improvements: Train: {(epoch_losses[epoch-1] - epoch_losses[epoch]):.2f}|\")\n",
    "        #  Test: {(validation_losses[epoch-1] - validation_losses[epoch]):.2f}\n",
    "    \n",
    "    scheduler.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000002178B25DD90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "validation = model(src_example.to(device), tgt_sos.to(device))\n",
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create some sample data for a test forward pass\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# batch_sizex5x5x5 tensor with random integers from 0 to vocab size\n",
    "src_example = torch.randint(0, vocab_size - 1, size=src_shape, dtype=torch.int).view(-1).unsqueeze(0).expand(batch_size,-1).to(device)\n",
    "tgt_example = torch.cat([torch.tensor([vocab_size-1]).view(-1),torch.randint(0, vocab_size - 1, size=tgt_shape, dtype=torch.int).view(-1)]).unsqueeze(0).expand(batch_size,-1).to(device)\n",
    "tgt_sos = torch.tensor([vocab_size-1]).unsqueeze(0).expand(batch_size,-1).to(device)\n",
    "\n",
    "output = model(src_example, tgt_sos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
