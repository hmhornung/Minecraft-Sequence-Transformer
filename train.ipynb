{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from MinecraftSequencePredict import MinecraftSequencePredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "#      Hyperparameters\n",
    "# -----------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = 250\n",
    "d_model = 768\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "\n",
    "src_shape  = (5,5,5)\n",
    "tgt_shape  = (1,5,5)\n",
    "tgt_offset = (5,0,0)\n",
    "# Model Definition\n",
    "model = MinecraftSequencePredict(vocab_size, \n",
    "                                 d_model, \n",
    "                                 nhead, \n",
    "                                 num_encoder_layers, \n",
    "                                 num_decoder_layers, \n",
    "                                 src_shape, \n",
    "                                 tgt_shape, \n",
    "                                 tgt_offset, \n",
    "                                 device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "learning_rate = 5e-4\n",
    "eps = 1e-9\n",
    "\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create some sample data for a test forward pass\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# batch_sizex5x5x5 tensor with random integers from 0 to vocab size\n",
    "src_example = torch.randint(0, vocab_size - 1, size=src_shape, dtype=torch.int).view(-1).unsqueeze(0).expand(batch_size,-1).to(device)\n",
    "tgt_example = torch.cat([torch.tensor([vocab_size-1]).view(-1),torch.randint(0, vocab_size - 1, size=tgt_shape, dtype=torch.int).view(-1)]).unsqueeze(0).expand(batch_size,-1).to(device)\n",
    "tgt_sos = torch.tensor([vocab_size-1]).unsqueeze(0).expand(batch_size,-1).to(device)\n",
    "\n",
    "output = model(src_example, tgt_sos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss: 5.521764755249023\n",
      "batch loss: 5.510337829589844\n",
      "batch loss: 5.496887683868408\n",
      "batch loss: 5.488706588745117\n",
      "batch loss: 5.484743595123291\n",
      "batch loss: 5.484691143035889\n",
      "batch loss: 5.483263969421387\n",
      "batch loss: 5.461836814880371\n",
      "batch loss: 5.453036308288574\n",
      "batch loss: 5.451832294464111\n",
      "epoch 1 loss: 54.837100982666016\n",
      "batch loss: 5.448557376861572\n",
      "batch loss: 5.445276737213135\n",
      "batch loss: 5.4478936195373535\n",
      "batch loss: 5.44763708114624\n",
      "batch loss: 5.437561988830566\n",
      "batch loss: 5.433464527130127\n",
      "batch loss: 5.398129940032959\n",
      "batch loss: 5.397557258605957\n",
      "batch loss: 5.355498313903809\n",
      "batch loss: 5.347979545593262\n",
      "epoch 2 loss: 54.1595573425293\n",
      "batch loss: 5.335546493530273\n",
      "batch loss: 5.319689750671387\n",
      "batch loss: 5.311722755432129\n",
      "batch loss: 5.287668228149414\n",
      "batch loss: 5.26289701461792\n",
      "batch loss: 5.235231876373291\n",
      "batch loss: 5.1971259117126465\n",
      "batch loss: 5.134206771850586\n",
      "batch loss: 5.086359977722168\n",
      "batch loss: 5.053767681121826\n",
      "epoch 3 loss: 52.224220275878906\n",
      "batch loss: 4.9894633293151855\n",
      "batch loss: 4.937957763671875\n",
      "batch loss: 4.889473915100098\n",
      "batch loss: 4.859219551086426\n",
      "batch loss: 4.8400163650512695\n",
      "batch loss: 4.819319725036621\n",
      "batch loss: 4.811931133270264\n",
      "batch loss: 4.80057430267334\n",
      "batch loss: 4.787632465362549\n",
      "batch loss: 4.758633136749268\n",
      "epoch 4 loss: 48.494224548339844\n",
      "batch loss: 4.7265119552612305\n",
      "batch loss: 4.7060933113098145\n",
      "batch loss: 4.697566986083984\n",
      "batch loss: 4.6927571296691895\n",
      "batch loss: 4.68971586227417\n",
      "batch loss: 4.688337326049805\n",
      "batch loss: 4.687236309051514\n",
      "batch loss: 4.6861891746521\n",
      "batch loss: 4.685328960418701\n",
      "batch loss: 4.684814453125\n",
      "epoch 5 loss: 46.944549560546875\n",
      "batch loss: 4.6844000816345215\n",
      "batch loss: 4.6841559410095215\n",
      "batch loss: 4.683891773223877\n",
      "batch loss: 4.683618545532227\n",
      "batch loss: 4.683350563049316\n",
      "batch loss: 4.683035850524902\n",
      "batch loss: 4.682851314544678\n",
      "batch loss: 4.6824421882629395\n",
      "batch loss: 4.681949138641357\n",
      "batch loss: 4.680306434631348\n",
      "epoch 6 loss: 46.83000183105469\n",
      "batch loss: 4.675617694854736\n",
      "batch loss: 4.6554388999938965\n",
      "batch loss: 4.648638725280762\n",
      "batch loss: 4.652886390686035\n",
      "batch loss: 4.631760120391846\n",
      "batch loss: 4.614577770233154\n",
      "batch loss: 4.614478588104248\n",
      "batch loss: 4.6135783195495605\n",
      "batch loss: 4.609787464141846\n",
      "batch loss: 4.608356952667236\n",
      "epoch 7 loss: 46.32511901855469\n",
      "batch loss: 4.607922077178955\n",
      "batch loss: 4.60786771774292\n",
      "batch loss: 4.607522964477539\n",
      "batch loss: 4.607195854187012\n",
      "batch loss: 4.606773376464844\n",
      "batch loss: 4.6064653396606445\n",
      "batch loss: 4.606037616729736\n",
      "batch loss: 4.6057658195495605\n",
      "batch loss: 4.605209827423096\n",
      "batch loss: 4.603859901428223\n",
      "epoch 8 loss: 46.06461715698242\n",
      "batch loss: 4.600235939025879\n",
      "batch loss: 4.5861711502075195\n",
      "batch loss: 4.57291841506958\n",
      "batch loss: 4.579017162322998\n",
      "batch loss: 4.570607662200928\n",
      "batch loss: 4.568362712860107\n",
      "batch loss: 4.568572521209717\n",
      "batch loss: 4.5688395500183105\n",
      "batch loss: 4.567863464355469\n",
      "batch loss: 4.563521385192871\n",
      "epoch 9 loss: 45.74611282348633\n",
      "batch loss: 4.546212196350098\n",
      "batch loss: 4.53292179107666\n",
      "batch loss: 4.536294460296631\n",
      "batch loss: 4.535861492156982\n",
      "batch loss: 4.531648635864258\n",
      "batch loss: 4.530449867248535\n",
      "batch loss: 4.5302958488464355\n",
      "batch loss: 4.530428886413574\n",
      "batch loss: 4.530409812927246\n",
      "batch loss: 4.530152797698975\n",
      "epoch 10 loss: 45.334678649902344\n",
      "batch loss: 4.529862880706787\n",
      "batch loss: 4.529679298400879\n",
      "batch loss: 4.52951192855835\n",
      "batch loss: 4.529391288757324\n",
      "batch loss: 4.529271125793457\n",
      "batch loss: 4.529192924499512\n",
      "batch loss: 4.529121398925781\n",
      "batch loss: 4.529080390930176\n",
      "batch loss: 4.529025077819824\n",
      "batch loss: 4.528998374938965\n",
      "epoch 11 loss: 45.29313659667969\n",
      "batch loss: 4.528944969177246\n",
      "batch loss: 4.528933525085449\n",
      "batch loss: 4.528895378112793\n",
      "batch loss: 4.528874397277832\n",
      "batch loss: 4.528850078582764\n",
      "batch loss: 4.528828144073486\n",
      "batch loss: 4.52880334854126\n",
      "batch loss: 4.528785705566406\n",
      "batch loss: 4.5287699699401855\n",
      "batch loss: 4.52875280380249\n",
      "epoch 12 loss: 45.28843688964844\n",
      "batch loss: 4.528731346130371\n",
      "batch loss: 4.528720378875732\n",
      "batch loss: 4.5287089347839355\n",
      "batch loss: 4.5286946296691895\n",
      "batch loss: 4.528684139251709\n",
      "batch loss: 4.5286760330200195\n",
      "batch loss: 4.528664588928223\n",
      "batch loss: 4.528652667999268\n",
      "batch loss: 4.528645992279053\n",
      "batch loss: 4.5286455154418945\n",
      "epoch 13 loss: 45.28681945800781\n",
      "batch loss: 4.528635501861572\n",
      "batch loss: 4.528631210327148\n",
      "batch loss: 4.528622627258301\n",
      "batch loss: 4.528618812561035\n",
      "batch loss: 4.528614044189453\n",
      "batch loss: 4.528605937957764\n",
      "batch loss: 4.528605937957764\n",
      "batch loss: 4.528595924377441\n",
      "batch loss: 4.528593063354492\n",
      "batch loss: 4.528593063354492\n",
      "epoch 14 loss: 45.28611755371094\n",
      "batch loss: 4.5285868644714355\n",
      "batch loss: 4.528581142425537\n",
      "batch loss: 4.528579235076904\n",
      "batch loss: 4.528575897216797\n",
      "batch loss: 4.528573513031006\n",
      "batch loss: 4.528569221496582\n",
      "batch loss: 4.528567314147949\n",
      "batch loss: 4.528565406799316\n",
      "batch loss: 4.528558731079102\n",
      "batch loss: 4.528558731079102\n",
      "epoch 15 loss: 45.28571319580078\n",
      "batch loss: 4.528557777404785\n",
      "batch loss: 4.528558731079102\n",
      "batch loss: 4.5285539627075195\n",
      "batch loss: 4.52855110168457\n",
      "batch loss: 4.528547286987305\n",
      "batch loss: 4.528547286987305\n",
      "batch loss: 4.528545379638672\n",
      "batch loss: 4.5285444259643555\n",
      "batch loss: 4.5285420417785645\n",
      "batch loss: 4.528542518615723\n",
      "epoch 16 loss: 45.28548812866211\n",
      "batch loss: 4.528539657592773\n",
      "batch loss: 4.528538703918457\n",
      "batch loss: 4.528536796569824\n",
      "batch loss: 4.528534412384033\n",
      "batch loss: 4.528535842895508\n",
      "batch loss: 4.528531551361084\n",
      "batch loss: 4.528532028198242\n",
      "batch loss: 4.528528213500977\n",
      "batch loss: 4.528527736663818\n",
      "batch loss: 4.52852725982666\n",
      "epoch 17 loss: 45.28533172607422\n",
      "batch loss: 4.528527736663818\n",
      "batch loss: 4.5285234451293945\n",
      "batch loss: 4.528522968292236\n",
      "batch loss: 4.528522968292236\n",
      "batch loss: 4.5285210609436035\n",
      "batch loss: 4.528519630432129\n",
      "batch loss: 4.528520584106445\n",
      "batch loss: 4.528518199920654\n",
      "batch loss: 4.528515815734863\n",
      "batch loss: 4.528515815734863\n",
      "epoch 18 loss: 45.28520202636719\n",
      "batch loss: 4.528514385223389\n",
      "batch loss: 4.5285139083862305\n",
      "batch loss: 4.528512477874756\n",
      "batch loss: 4.5285115242004395\n",
      "batch loss: 4.528510093688965\n",
      "batch loss: 4.528510570526123\n",
      "batch loss: 4.528510570526123\n",
      "batch loss: 4.528509140014648\n",
      "batch loss: 4.528506755828857\n",
      "batch loss: 4.528507232666016\n",
      "epoch 19 loss: 45.28511047363281\n",
      "batch loss: 4.528506755828857\n",
      "batch loss: 4.528504371643066\n",
      "batch loss: 4.528504848480225\n",
      "batch loss: 4.52850341796875\n",
      "batch loss: 4.528501510620117\n",
      "batch loss: 4.528501033782959\n",
      "batch loss: 4.528501510620117\n",
      "batch loss: 4.528501510620117\n",
      "batch loss: 4.52849817276001\n",
      "batch loss: 4.528500556945801\n",
      "epoch 20 loss: 45.28502655029297\n",
      "batch loss: 4.528496742248535\n",
      "batch loss: 4.528496265411377\n",
      "batch loss: 4.528497695922852\n",
      "batch loss: 4.528498649597168\n",
      "batch loss: 4.528494358062744\n",
      "batch loss: 4.5284953117370605\n",
      "batch loss: 4.5284929275512695\n",
      "batch loss: 4.528493404388428\n",
      "batch loss: 4.528492450714111\n",
      "batch loss: 4.5284905433654785\n",
      "epoch 21 loss: 45.284950256347656\n",
      "batch loss: 4.5284905433654785\n",
      "batch loss: 4.528491020202637\n",
      "batch loss: 4.528489112854004\n",
      "batch loss: 4.528489112854004\n",
      "batch loss: 4.528486251831055\n",
      "batch loss: 4.528487205505371\n",
      "batch loss: 4.5284857749938965\n",
      "batch loss: 4.528485298156738\n",
      "batch loss: 4.528486251831055\n",
      "batch loss: 4.528483867645264\n",
      "epoch 22 loss: 45.28487777709961\n",
      "batch loss: 4.52848482131958\n",
      "batch loss: 4.528482437133789\n",
      "batch loss: 4.528483867645264\n",
      "batch loss: 4.528481483459473\n",
      "batch loss: 4.528482913970947\n",
      "batch loss: 4.5284810066223145\n",
      "batch loss: 4.528480052947998\n",
      "batch loss: 4.528478622436523\n",
      "batch loss: 4.528478622436523\n",
      "batch loss: 4.528478622436523\n",
      "epoch 23 loss: 45.284812927246094\n",
      "batch loss: 4.528479099273682\n",
      "batch loss: 4.528477191925049\n",
      "batch loss: 4.528478145599365\n",
      "batch loss: 4.528476715087891\n",
      "batch loss: 4.528478145599365\n",
      "batch loss: 4.528475761413574\n",
      "batch loss: 4.528475761413574\n",
      "batch loss: 4.528475761413574\n",
      "batch loss: 4.528472900390625\n",
      "batch loss: 4.528473377227783\n",
      "epoch 24 loss: 45.28476333618164\n",
      "batch loss: 4.528472423553467\n",
      "batch loss: 4.528470993041992\n",
      "batch loss: 4.528470039367676\n",
      "batch loss: 4.528471946716309\n",
      "batch loss: 4.528469562530518\n",
      "batch loss: 4.528470516204834\n",
      "batch loss: 4.528468132019043\n",
      "batch loss: 4.528469562530518\n",
      "batch loss: 4.528466701507568\n",
      "batch loss: 4.52846622467041\n",
      "epoch 25 loss: 45.28469467163086\n",
      "batch loss: 4.528465747833252\n",
      "batch loss: 4.528468132019043\n",
      "batch loss: 4.528465747833252\n",
      "batch loss: 4.528465747833252\n",
      "batch loss: 4.52846622467041\n",
      "batch loss: 4.52846622467041\n",
      "batch loss: 4.5284647941589355\n",
      "batch loss: 4.528464317321777\n",
      "batch loss: 4.528463840484619\n",
      "batch loss: 4.5284624099731445\n",
      "epoch 26 loss: 45.28465270996094\n",
      "batch loss: 4.528463363647461\n",
      "batch loss: 4.528463363647461\n",
      "batch loss: 4.52846097946167\n",
      "batch loss: 4.528461456298828\n",
      "batch loss: 4.528461933135986\n",
      "batch loss: 4.528460502624512\n",
      "batch loss: 4.5284600257873535\n",
      "batch loss: 4.528459072113037\n",
      "batch loss: 4.528458595275879\n",
      "batch loss: 4.528456687927246\n",
      "epoch 27 loss: 45.28460693359375\n",
      "batch loss: 4.528456687927246\n",
      "batch loss: 4.528458118438721\n",
      "batch loss: 4.528457164764404\n",
      "batch loss: 4.528458118438721\n",
      "batch loss: 4.52845573425293\n",
      "batch loss: 4.52845573425293\n",
      "batch loss: 4.52845573425293\n",
      "batch loss: 4.528454780578613\n",
      "batch loss: 4.528454780578613\n",
      "batch loss: 4.52845573425293\n",
      "epoch 28 loss: 45.28456115722656\n",
      "batch loss: 4.5284528732299805\n",
      "batch loss: 4.528454780578613\n",
      "batch loss: 4.528453826904297\n",
      "batch loss: 4.528451919555664\n",
      "batch loss: 4.5284528732299805\n",
      "batch loss: 4.5284528732299805\n",
      "batch loss: 4.528450965881348\n",
      "batch loss: 4.528451919555664\n",
      "batch loss: 4.5284504890441895\n",
      "batch loss: 4.528450012207031\n",
      "epoch 29 loss: 45.284523010253906\n",
      "batch loss: 4.5284504890441895\n",
      "batch loss: 4.528448581695557\n",
      "batch loss: 4.528449058532715\n",
      "batch loss: 4.528449058532715\n",
      "batch loss: 4.52844762802124\n",
      "batch loss: 4.52844762802124\n",
      "batch loss: 4.528448104858398\n",
      "batch loss: 4.52844762802124\n",
      "batch loss: 4.528447151184082\n",
      "batch loss: 4.528446674346924\n",
      "epoch 30 loss: 45.284481048583984\n",
      "batch loss: 4.528444766998291\n",
      "batch loss: 4.528446674346924\n",
      "batch loss: 4.528445720672607\n",
      "batch loss: 4.528446674346924\n",
      "batch loss: 4.528444766998291\n",
      "batch loss: 4.528445720672607\n",
      "batch loss: 4.528444290161133\n",
      "batch loss: 4.528444290161133\n",
      "batch loss: 4.528443336486816\n",
      "batch loss: 4.528442859649658\n",
      "epoch 31 loss: 45.284446716308594\n",
      "batch loss: 4.528441905975342\n",
      "batch loss: 4.528442859649658\n",
      "batch loss: 4.528441429138184\n",
      "batch loss: 4.528441429138184\n",
      "batch loss: 4.528440952301025\n",
      "batch loss: 4.528440952301025\n",
      "batch loss: 4.528440475463867\n",
      "batch loss: 4.528439998626709\n",
      "batch loss: 4.528439521789551\n",
      "batch loss: 4.528439998626709\n",
      "epoch 32 loss: 45.28440475463867\n",
      "batch loss: 4.528440475463867\n",
      "batch loss: 4.528438091278076\n",
      "batch loss: 4.528438091278076\n",
      "batch loss: 4.528438091278076\n",
      "batch loss: 4.528437614440918\n",
      "batch loss: 4.528438568115234\n",
      "batch loss: 4.528438091278076\n",
      "batch loss: 4.528436183929443\n",
      "batch loss: 4.5284342765808105\n",
      "batch loss: 4.528437614440918\n",
      "epoch 33 loss: 45.28437805175781\n",
      "batch loss: 4.528435707092285\n",
      "batch loss: 4.528436660766602\n",
      "batch loss: 4.5284342765808105\n",
      "batch loss: 4.528435230255127\n",
      "batch loss: 4.5284342765808105\n",
      "batch loss: 4.528433322906494\n",
      "batch loss: 4.528435230255127\n",
      "batch loss: 4.528433322906494\n",
      "batch loss: 4.528432846069336\n",
      "batch loss: 4.528433799743652\n",
      "epoch 34 loss: 45.28434753417969\n",
      "batch loss: 4.528432846069336\n",
      "batch loss: 4.528432369232178\n",
      "batch loss: 4.5284318923950195\n",
      "batch loss: 4.5284318923950195\n",
      "batch loss: 4.528430461883545\n",
      "batch loss: 4.5284318923950195\n",
      "batch loss: 4.5284318923950195\n",
      "batch loss: 4.528430461883545\n",
      "batch loss: 4.5284318923950195\n",
      "batch loss: 4.528430938720703\n",
      "epoch 35 loss: 45.28431701660156\n",
      "batch loss: 4.528431415557861\n",
      "batch loss: 4.528430938720703\n",
      "batch loss: 4.528427600860596\n",
      "batch loss: 4.5284295082092285\n",
      "batch loss: 4.528428554534912\n",
      "batch loss: 4.528427600860596\n",
      "batch loss: 4.5284295082092285\n",
      "batch loss: 4.528428077697754\n",
      "batch loss: 4.528428077697754\n",
      "batch loss: 4.528426170349121\n",
      "epoch 36 loss: 45.28428649902344\n",
      "batch loss: 4.528426647186279\n",
      "batch loss: 4.5284271240234375\n",
      "batch loss: 4.528428077697754\n",
      "batch loss: 4.528426170349121\n",
      "batch loss: 4.528425693511963\n",
      "batch loss: 4.528425216674805\n",
      "batch loss: 4.528425216674805\n",
      "batch loss: 4.528425216674805\n",
      "batch loss: 4.528425216674805\n",
      "batch loss: 4.5284247398376465\n",
      "epoch 37 loss: 45.28425979614258\n",
      "batch loss: 4.5284247398376465\n",
      "batch loss: 4.528425216674805\n",
      "batch loss: 4.52842378616333\n",
      "batch loss: 4.528424263000488\n",
      "batch loss: 4.52842378616333\n",
      "batch loss: 4.528422832489014\n",
      "batch loss: 4.528422832489014\n",
      "batch loss: 4.5284223556518555\n",
      "batch loss: 4.5284223556518555\n",
      "batch loss: 4.5284223556518555\n",
      "epoch 38 loss: 45.284236907958984\n",
      "batch loss: 4.528422832489014\n",
      "batch loss: 4.5284223556518555\n",
      "batch loss: 4.5284223556518555\n",
      "batch loss: 4.528420448303223\n",
      "batch loss: 4.528421401977539\n",
      "batch loss: 4.528421401977539\n",
      "batch loss: 4.528420448303223\n",
      "batch loss: 4.5284199714660645\n",
      "batch loss: 4.5284199714660645\n",
      "batch loss: 4.528420448303223\n",
      "epoch 39 loss: 45.284210205078125\n",
      "batch loss: 4.5284199714660645\n",
      "batch loss: 4.528419017791748\n",
      "batch loss: 4.528420448303223\n",
      "batch loss: 4.528419017791748\n",
      "batch loss: 4.5284199714660645\n",
      "batch loss: 4.528417587280273\n",
      "batch loss: 4.528417587280273\n",
      "batch loss: 4.528417587280273\n",
      "batch loss: 4.528417587280273\n",
      "batch loss: 4.528417587280273\n",
      "epoch 40 loss: 45.28418731689453\n",
      "batch loss: 4.528416156768799\n",
      "batch loss: 4.528417110443115\n",
      "batch loss: 4.528416633605957\n",
      "batch loss: 4.528416633605957\n",
      "batch loss: 4.528416156768799\n",
      "batch loss: 4.528416633605957\n",
      "batch loss: 4.528416633605957\n",
      "batch loss: 4.528414726257324\n",
      "batch loss: 4.528414726257324\n",
      "batch loss: 4.528414726257324\n",
      "epoch 41 loss: 45.28416061401367\n",
      "batch loss: 4.528415679931641\n",
      "batch loss: 4.528415203094482\n",
      "batch loss: 4.528414726257324\n",
      "batch loss: 4.528413772583008\n",
      "batch loss: 4.528414249420166\n",
      "batch loss: 4.528414726257324\n",
      "batch loss: 4.528413772583008\n",
      "batch loss: 4.52841329574585\n",
      "batch loss: 4.528412818908691\n",
      "batch loss: 4.528412342071533\n",
      "epoch 42 loss: 45.28413772583008\n",
      "batch loss: 4.528412342071533\n",
      "batch loss: 4.528412818908691\n",
      "batch loss: 4.52841329574585\n",
      "batch loss: 4.528411388397217\n",
      "batch loss: 4.528411388397217\n",
      "batch loss: 4.528411865234375\n",
      "batch loss: 4.528412342071533\n",
      "batch loss: 4.528412342071533\n",
      "batch loss: 4.528409957885742\n",
      "batch loss: 4.528411865234375\n",
      "epoch 43 loss: 45.28411865234375\n",
      "batch loss: 4.528409957885742\n",
      "batch loss: 4.528411388397217\n",
      "batch loss: 4.528409957885742\n",
      "batch loss: 4.528409957885742\n",
      "batch loss: 4.5284104347229\n",
      "batch loss: 4.528409957885742\n",
      "batch loss: 4.528409004211426\n",
      "batch loss: 4.528409004211426\n",
      "batch loss: 4.528407573699951\n",
      "batch loss: 4.528408527374268\n",
      "epoch 44 loss: 45.284095764160156\n",
      "batch loss: 4.528408527374268\n",
      "batch loss: 4.528408527374268\n",
      "batch loss: 4.528407096862793\n",
      "batch loss: 4.528409004211426\n",
      "batch loss: 4.528407573699951\n",
      "batch loss: 4.528408050537109\n",
      "batch loss: 4.528407573699951\n",
      "batch loss: 4.528407573699951\n",
      "batch loss: 4.528407096862793\n",
      "batch loss: 4.52840518951416\n",
      "epoch 45 loss: 45.28407669067383\n",
      "batch loss: 4.528406143188477\n",
      "batch loss: 4.528407096862793\n",
      "batch loss: 4.528406143188477\n",
      "batch loss: 4.528406143188477\n",
      "batch loss: 4.528406143188477\n",
      "batch loss: 4.528405666351318\n",
      "batch loss: 4.528406143188477\n",
      "batch loss: 4.52840518951416\n",
      "batch loss: 4.52840518951416\n",
      "batch loss: 4.528404235839844\n",
      "epoch 46 loss: 45.2840576171875\n",
      "batch loss: 4.528405666351318\n",
      "batch loss: 4.52840518951416\n",
      "batch loss: 4.528404712677002\n",
      "batch loss: 4.52840518951416\n",
      "batch loss: 4.5284037590026855\n",
      "batch loss: 4.528404712677002\n",
      "batch loss: 4.528404235839844\n",
      "batch loss: 4.528403282165527\n",
      "batch loss: 4.5284037590026855\n",
      "batch loss: 4.528403282165527\n",
      "epoch 47 loss: 45.2840461730957\n",
      "batch loss: 4.528403282165527\n",
      "batch loss: 4.5284037590026855\n",
      "batch loss: 4.528402328491211\n",
      "batch loss: 4.528402805328369\n",
      "batch loss: 4.528402328491211\n",
      "batch loss: 4.528403282165527\n",
      "batch loss: 4.528401851654053\n",
      "batch loss: 4.528402328491211\n",
      "batch loss: 4.528402328491211\n",
      "batch loss: 4.528400897979736\n",
      "epoch 48 loss: 45.28402328491211\n",
      "batch loss: 4.528402328491211\n",
      "batch loss: 4.528401851654053\n",
      "batch loss: 4.5284013748168945\n",
      "batch loss: 4.528400421142578\n",
      "batch loss: 4.52839994430542\n",
      "batch loss: 4.528400897979736\n",
      "batch loss: 4.528400897979736\n",
      "batch loss: 4.52839994430542\n",
      "batch loss: 4.528399467468262\n",
      "batch loss: 4.5283989906311035\n",
      "epoch 49 loss: 45.28400802612305\n",
      "batch loss: 4.528398513793945\n",
      "batch loss: 4.528398513793945\n",
      "batch loss: 4.528400421142578\n",
      "batch loss: 4.528398036956787\n",
      "batch loss: 4.528399467468262\n",
      "batch loss: 4.528398513793945\n",
      "batch loss: 4.528398036956787\n",
      "batch loss: 4.528398513793945\n",
      "batch loss: 4.528397560119629\n",
      "batch loss: 4.528398036956787\n",
      "epoch 50 loss: 45.28398132324219\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=eps)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "batch_losses = []\n",
    "epoch_losses = []\n",
    "\n",
    "# this is just an example until a Dataloader is implemented\n",
    "num_batches = 10 \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        training_src_batch = src_example.to(device) # Batch Size x Src Sequence Length\n",
    "        training_tgt_batch = tgt_example.to(device) # Batch Size x Tgt Sequence Length\n",
    "        \n",
    "        output = model(training_src_batch, training_tgt_batch) # Batch Size x Tgt Sequence Length x Vocab Size\n",
    "        \n",
    "        # Batch Size x Tgt Sequence Length x Vocab Size --> (Batch Size * Tgt Sequence Length x Vocab Size)\n",
    "        loss = loss_fn(output.view(-1, vocab_size), training_tgt_batch.view(-1))\n",
    "        batch_losses.append(loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        print(f'batch loss: {batch_losses[len(batch_losses)-1]}')\n",
    "    \n",
    "    epoch_losses.append(sum(batch_losses))\n",
    "    \n",
    "    print(f'epoch {epoch + 1} loss: {epoch_losses[len(epoch_losses)-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000002178B25DD90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "validation = model(src_example.to(device), tgt_sos.to(device))\n",
    "model.parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
