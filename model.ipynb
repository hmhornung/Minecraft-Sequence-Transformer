{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PositionalEmbedding3D import PositionalEmbedding3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sizes:\n",
      "src: torch.Size([32, 125])\n",
      "tgt: torch.Size([32, 26])\n",
      "\n",
      "sizes after block embeddings:\n",
      "src: torch.Size([32, 125, 768])\n",
      "tgt: torch.Size([32, 26, 768])\n",
      "\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "positional embedding shape:\n",
      "torch.Size([125, 768])\n",
      "x shape:\n",
      "torch.Size([32, 125, 768])\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "positional embedding shape:\n",
      "torch.Size([26, 768])\n",
      "x shape:\n",
      "torch.Size([32, 26, 768])\n",
      "sizes after positional embeddings:\n",
      "src: torch.Size([32, 125, 768])\n",
      "tgt: torch.Size([32, 26, 768])\n",
      "\n",
      "output shape from transformer:\n",
      "torch.Size([32, 26, 768])\n",
      "\n",
      "output shape after linear layer:\n",
      "torch.Size([32, 26, 255])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MinecraftSequencePredict(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, src_shape: tuple, tgt_shape: tuple, tgt_offset: tuple, device):\n",
    "        super(MinecraftSequencePredict, self).__init__()\n",
    "\n",
    "        # Define the embedding layer\n",
    "        self.block_embedding = nn.Embedding(vocab_size, d_model, device=device)\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding3D(d_model, src_shape, tgt_shape, tgt_offset, device=device)\n",
    "        # Define the transformer model\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            batch_first=True,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Define the output layer\n",
    "        self.fc = nn.Linear(d_model, vocab_size,device=device)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        print(f'sizes:\\nsrc: {src.shape}\\ntgt: {tgt.shape}\\n')\n",
    "        \n",
    "        src = self.block_embedding(src)\n",
    "        tgt = self.block_embedding(tgt)\n",
    "        \n",
    "        print(f'sizes after block embeddings:\\nsrc: {src.shape}\\ntgt: {tgt.shape}\\n')\n",
    "        \n",
    "        src = self.pos_embedding(src, True)\n",
    "        tgt = self.pos_embedding(tgt, False)\n",
    "        \n",
    "        print(f'sizes after positional embeddings:\\nsrc: {src.shape}\\ntgt: {tgt.shape}\\n')\n",
    "        \n",
    "        output = self.transformer(src, tgt)\n",
    "        \n",
    "        print(f'output shape from transformer:\\n{output.shape}\\n')\n",
    "\n",
    "        # You may want to reshape or slice the output based on your specific task\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        print(f'output shape after linear layer:\\n{output.shape}\\n')\n",
    "\n",
    "        return output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Example usage\n",
    "vocab_size = 255  # Adjust based on your vocabulary size\n",
    "d_model = 768\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "\n",
    "batch_size = 32\n",
    "src_shape  = (5,5,5)\n",
    "tgt_shape  = (1,5,5)\n",
    "tgt_offset = (5,0,0)\n",
    "\n",
    "\n",
    "# Set the seed for reproducibility (optional)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a 5x5x5 tensor with random integers from 0 to 255\n",
    "src = torch.randint(0, vocab_size - 1, size=src_shape, dtype=torch.int).view(-1).unsqueeze(0).expand(batch_size,-1).to(device)\n",
    "tgt = torch.cat([torch.tensor([vocab_size-1]).view(-1),torch.randint(0, vocab_size - 1, size=tgt_shape, dtype=torch.int).view(-1)]).unsqueeze(0).expand(batch_size,-1).to(device)\n",
    "tgt_sos = torch.tensor([vocab_size-1]).unsqueeze(0)\n",
    "# print(src.device)\n",
    "# print(tgt.device)\n",
    "\n",
    "\n",
    "model = MinecraftSequencePredict(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, src_shape, tgt_shape, tgt_offset, device).to(device)\n",
    "\n",
    "output = model(src, tgt)\n",
    "# print(model.eval)\n",
    "# torch.argmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[170,  67, 148, 136, 200, 159, 242, 104,  24, 201,  84, 236,  94, 110,\n",
      "         185, 182,  31, 175, 217, 132, 223, 102,   9,  79, 247,  61,   1,  97,\n",
      "         143, 231, 142, 112, 109,  23, 137, 120, 155, 250, 188, 182, 122,  64,\n",
      "          93, 183,  89, 201, 177,  24, 111,  55, 156,  11, 232,  51, 170, 189,\n",
      "         157,  62, 202, 130,  86, 232,   2,  16,  58, 107,  37,   6,  37, 181,\n",
      "         131,  15, 190, 141,  22, 131,  73, 203,  92, 157, 132, 179, 220, 117,\n",
      "         138,  78, 183,   2, 133,  81,  73, 209,  77, 205, 157,  37,  15, 135,\n",
      "          59, 120, 206, 205,  11, 131,  65, 187,  17,  73,  17, 242, 131, 106,\n",
      "         253, 195, 246,  77, 172, 119, 155,  46, 167, 240, 159, 242, 202]],\n",
      "       dtype=torch.int32)\n",
      "tensor([[254,  77,  41, 252, 145, 107, 213, 136, 208, 138, 136, 154, 176, 248,\n",
      "          12,  22,   3, 242,  17, 129,  85,  62,  78, 212,  10, 175]])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "vocab_size = 255  # Adjust based on your vocabulary size\n",
    "d_model = 768\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "\n",
    "src_shape  = (5,5,5)\n",
    "tgt_shape  = (1,5,5)\n",
    "tgt_offset = (5,0,0)\n",
    "\n",
    "\n",
    "# Set the seed for reproducibility (optional)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a 5x5x5 tensor with random integers from 0 to 255\n",
    "src = torch.randint(0, vocab_size - 1, size=src_shape, dtype=torch.int).ravel().unsqueeze(0)\n",
    "tgt = torch.cat([torch.tensor([vocab_size-1]),torch.randint(0, vocab_size - 1, size=tgt_shape, dtype=torch.int).ravel()]).unsqueeze(0)\n",
    "\n",
    "print(src)\n",
    "print(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 0) (0, 0, 1) (0, 0, 2) (0, 0, 3) (0, 1, 0) (0, 1, 1) (0, 1, 2)\n",
      " (0, 1, 3) (0, 2, 0) (0, 2, 1) (0, 2, 2) (0, 2, 3) (0, 3, 0) (0, 3, 1)\n",
      " (0, 3, 2) (0, 3, 3) (1, 0, 0) (1, 0, 1) (1, 0, 2) (1, 0, 3) (1, 1, 0)\n",
      " (1, 1, 1) (1, 1, 2) (1, 1, 3) (1, 2, 0) (1, 2, 1) (1, 2, 2) (1, 2, 3)\n",
      " (1, 3, 0) (1, 3, 1) (1, 3, 2) (1, 3, 3) (2, 0, 0) (2, 0, 1) (2, 0, 2)\n",
      " (2, 0, 3) (2, 1, 0) (2, 1, 1) (2, 1, 2) (2, 1, 3) (2, 2, 0) (2, 2, 1)\n",
      " (2, 2, 2) (2, 2, 3) (2, 3, 0) (2, 3, 1) (2, 3, 2) (2, 3, 3) (3, 0, 0)\n",
      " (3, 0, 1) (3, 0, 2) (3, 0, 3) (3, 1, 0) (3, 1, 1) (3, 1, 2) (3, 1, 3)\n",
      " (3, 2, 0) (3, 2, 1) (3, 2, 2) (3, 2, 3) (3, 3, 0) (3, 3, 1) (3, 3, 2)\n",
      " (3, 3, 3)]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# position = torch.arange(0, x.size(1)).unsqueeze(0)\n",
    "shape = (4,4,4)\n",
    "\n",
    "array_3d = np.empty(shape, dtype=object)\n",
    "\n",
    "# Fill in the array with tuples of indices\n",
    "for i in range(shape[0]):\n",
    "    for j in range(shape[1]):\n",
    "        for k in range(shape[2]):\n",
    "            array_3d[i, j, k] = (i, j, k)\n",
    "array_3d = array_3d.ravel()\n",
    "print(array_3d)\n",
    "arr = [i[0] for _, i in np.ndenumerate(array_3d)]\n",
    "arr.insert(0, 12)\n",
    "print(type(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=type), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m src_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m src_positions \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(src_shape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m      5\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(src_shape[\u001b[38;5;241m1\u001b[39m]):\n",
      "\u001b[1;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=type), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "src_shape = (5,5,5)\n",
    "src_positions = np.empty(src_shape, dtype=object)\n",
    "\n",
    "for i in range(src_shape[0]):\n",
    "            for j in range(src_shape[1]):\n",
    "                for k in range(src_shape[2]):\n",
    "                    src_positions[i, j, k] = (i, j, k)\n",
    "                    \n",
    "# src_positions = torch.from_numpy(dtype=np.object_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
